Today I'll be finishing up my listen-through about Hopfield networks (class 3) and moving on to class 4
NOTES:
console mode example for hotfield network
creates hopfield of size 4, training with two boolean patterns, one to work and one to verify
then displays pattern and training network with first pattern
then pattern 1 is presented to the network, which should present it back
then present pattern 2 and should get back pattern 1 since it's close enough to be recognized
more complex example
pattern drawing, will recall trained patterns if they're close to the current pattern
Class 3/16, Part 4/5
supervised learning, network is provided with a set of inputs, the input and the expected outputs are provided, trained by calculating drift from expected output and minimizing it
keeps going through to minimize the error rates 
unsupervised learning, provide the network with inputs but not outputs
used with large amounts of data that you want the network to group, like optical character recognition
adjusts weights using specific rules and algorithms over an arbitrary number of epochs
hebb's rule?
point of rules are to maximize groups
error calculation, difference between expected and produced for supervised, unsupervised done by how evenly the data is distributed?
can use the root mean square equation to calculate error
this can calculate difference between actual and ideal, plugging in actual-ideal
Class 3/16, Part 5/5
delta and hebbs rule
BELL RINGS
hebbs rule reinforces what the network knows
delta rule works for backpropagation, used to train the network to recognize a pattern
hebbs rule is delta weight ij = learning rate of a sub i and a sub j
learning rule would be rate*input*output
hebbs rule reinforced the results and didn't effect zeroes
delta rule explores dif between output and expected
weight change = 2 * learning rate * x sub i (ideal-actual)
supervised training algorithm dt ideal inclusion
rate*input*(ideal-actual)
constant 2 is often built into learning rate
Class 4/16, part 1/5
backpropagation and feedforward, 
backpropagation is a training method, along with simulated and genetic algorithms 
activation functions as well, a way that the output is scaled to appropriate values
as well as sigmoidal and hyperbolic tangent
example will be XOR, common example
very simple 
number of hidden layers is mostly trial and error
examine hidden layers
later will learn to prune hidden layer neurons
feedforward neural network simple, always goes forward from input to output
hidden layers are between inputs and outputs
xor operator: not the same
feedforward class later, can add layers to it with a number of neurons parameter
then a trainer, with training values, momentum (how much of previous traing applied to this training)
loops over iterations, displaying epoch number, until set number of epochs or low error rate
then works with training data 
keeps going to get as close as possible to a zero error rate
should get to almost perfect, aka desired input
Class 4/16, Part 2/5
activation functions
used to make sure the numbers are in a desireable range
bunch of possible functions
first, linear function of f(x) = x, mostly theoretical and not really an activation function, can't be used for backprogation because it has no useful derivative
does pass through negative and positive values
need to make sure your activation function fits the desired range of numbers you need
sigmoidal activation function is f(x) = 1/1+e^-x
default activation function for feedforward network example, and if you don't specify for his examples you use sigmoidal
sigmoid function has no negative y values, don't use it for things with negative outputs like stock prices
hyperbolic tangent function is f(x) = (e^2x - 1)/(e^2x +1), can be used for negatives
can be used for backpropagation along with sigmoidal
as x grows for hyperbolic functions, y flattens
use hyperbolic for positive and negative and sigmoidal for positive
MOVING TO END OF CLASS JOURNAL

Today I finished my analysis of the java implementation of Hopfield networks and moved on to evaluating the pros and cons of different activation functions
I plan to continue watching these videos so I can eventually examine his genetic algorithm function in greater detail
